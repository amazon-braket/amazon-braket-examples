{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd80a24",
   "metadata": {},
   "source": [
    "# Parallelize training for Quantum machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2968c01",
   "metadata": {},
   "source": [
    "Quantum machine learning (QML) is a special type of hybrid quantum-classical workload. Like classical machine learning (ML), there is usually a parameterized model, a dataset and a loss function. When training the model with the dataset, the parameters of the model are updated to minimize the loss function. In QML, the model contains one or many quantum circuits. The model may or may not also include classical neural nets. A loss function is usually defined for a single data point. Say a dataset $D$ has $N_D$ data points, $d_1$, $d_1$, ..., $d_{N_D}$. The losses associated with the data points are $L(d_1)$, $L(d_1)$, ..., $L(d_{N_D})$, where $L$ is the loss function. Without invoking any advanced feature, the algorithm script would compute these losses in serial, and then average them to be the total loss for gradient computations. This procedure is time consuming, especially when there are hundreds of data points. \n",
    "\n",
    "## Data parallelism\n",
    "The loss from one data point is independent of the other data points. The order of the loss evaluations therefore does not need to follow a specific order. They can even be evaluated <i>in parallel!</i> Losses and gradients of variational parameters associated with different data points can be evaluated on different GPUs at the same time. This is known as data parallelism. In this notebook, we will learn to use [SageMaker's distributed data parallel library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html) in Braket Jobs to accelerate the training of your quantum model. We go through examples to show you how to parallelize trainings across multiple GPUs in an instance, and even multiple GPUs over multiple instances! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8bf11",
   "metadata": {},
   "source": [
    "## Binary Classification of Sonar dataset\n",
    "Let's use a binary classification of the [Sonar dataset](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29) as the QML example. The Sonar dataset contains 208 data points each with 60 features that are collected from sonar signals bouncing off materials. Each data point is either labeled as \"M\" for mines or \"R\" for rocks. Our QML model consists of an input layer, a quantum circuit and an output layer. The input and output layer are classical dense layers. The dimension of classical input layer is $60\\times N$, where $N$ is the number of qubits in the quantum circuit. The result of the input layer is encoded into the quantum circuit using [angle embedding](https://pennylane.readthedocs.io/en/stable/code/api/pennylane.AngleEmbedding.html). After the angle embedding, the quantum circuit has the same structure as figure 4 of [this paper](https://arxiv.org/abs/1804.00633). It is a generic circuit ansatz that has two parametrized [strongly entangling layers](https://pennylane.readthedocs.io/en/user-docs-refactor/code/pennylane.templates.layers.StronglyEntanglingLayer.html) and a single parametrized rotation gate at the first qubit. The measurement is only performed at the first qubit. Using the concept of classical ML, the dimension of the quantum circuit layer is $N\\times1$. The classical output layer has dimension $1\\times1$ which takes the measurement of the quantum circuit and outputs a real number. The loss function is the [margin loss function](https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html) between the model output and the label. See [model_def.py](source_script/model_def.py) and [quantum_circuit.py](source_script/quantum_circuit.py) for more detail about the model and the quantum circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0b4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"data/sonar.all-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d34112",
   "metadata": {},
   "source": [
    "## Training with single GPU\n",
    "\n",
    "Let's start by running the job with `lightning.gpu` simulator on a single GPU in a `ml.p3.2xlarge` instance which has one Nvidia V100 GPU. The algorithm script to train our quantum model is [train_single.py](source_script/train_single.py). In the algorithm script, we use PennyLane with PyTorch as our framework, which are both included in Braket's pre-configured PyTorch container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337e3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "instance_config = InstanceConfig(instanceType='ml.p3.2xlarge')\n",
    "\n",
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"16\", \n",
    "                 \"batch_size\": \"16\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.01\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                }\n",
    "\n",
    "input_file_path = \"data/sonar.all-data\"\n",
    "\n",
    "image_uri=\"537332306153.dkr.ecr.us-west-2.amazonaws.com/lightning-bjob:v9\" # Remove this line after public container builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87147927",
   "metadata": {},
   "source": [
    "We submit our job after setting up instance configuration, hyperparameters and the job container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055c051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.gpu\",\n",
    "    source_module=\"source_script\",\n",
    "    entry_point=\"source_script.train_single\",\n",
    "    job_name=\"qml-single-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    instance_config=instance_config,\n",
    "    image_uri=image_uri,\n",
    "    wait_until_complete=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4f716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last loss': 0.09291739761829376}\n"
     ]
    }
   ],
   "source": [
    "# This cell should take about 6 minutes\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57df37",
   "metadata": {},
   "source": [
    "## Modify your algorithm script for data parallelism <a class=\"anchor\" id=\"modify\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94c5bd",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) has built-in features for data parallelism. With SageMaker's distributed data parallel library, Braket Jobs makes it easier for you to leverage data parallelism to accelerate your training. To use data parallelism, you need to slightly modify your algorithm script. As an example, we modify the algorithm script [train_single.py](source_script/train_single.py) to [train_dp.py](source_script/train_dp.py). Let's go through the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c1c13",
   "metadata": {},
   "source": [
    "First, we import the `smdistributed` package which does most of the heavy lifting for distributing your workloads across multiple GPUs and/or multiple instances. This package is pre-configured in the Braket PyTorch and TensorFlow containers. The `DDP` class from `smdistributed` converts the quantum model into a data parallelizable model. The `dist` module  tell our algorithm script the total number of instances for the training (`world_size`), and the `rank` and `local_rank` of a GPU. `rank` is the absolute index of a GPU across all instances, while `local_rank` is the index of a GPU within an instance. For example, if there are four instances each with eight GPUs allocated for the training, the `rank` ranges from 0 to 31 and the `local_rank` ranges from 0 to 7.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb5fbc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import smdistributed.dataparallel.torch.distributed as dist\n",
      "from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
      "    dp_info = {\n",
      "        \"world_size\": dist.get_world_size(),\n",
      "        \"rank\": dist.get_rank(),\n",
      "        \"local_rank\": dist.get_local_rank(),\n",
      "    }\n",
      "    batch_size //= dp_info[\"world_size\"] // 8\n",
      "    batch_size = max(batch_size, 1)\n",
      "    print(\"dp_info: \", dp_info)\n"
     ]
    }
   ],
   "source": [
    "!sed -n 23,24p source_script/train_dp.py\n",
    "!sed -n 65,72p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fb397",
   "metadata": {},
   "source": [
    "Next, we define a `DistributedSampler` according to the `world_size` and `rank`, and pass it into the data loader. This sampler avoids GPUs accessing the same slice of a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006010b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
      "        train_dataset, \n",
      "        num_replicas=dp_info[\"world_size\"], \n",
      "        rank=dp_info[\"rank\"]\n",
      "    )\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        train_dataset,\n",
      "        batch_size=batch_size,\n",
      "        shuffle=False,\n",
      "        num_workers=0,\n",
      "        pin_memory=True,\n",
      "        sampler=train_sampler,        \n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "!sed -n 81,93p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856c219",
   "metadata": {},
   "source": [
    "Next, we use the `DDP` class to make our quantum model parallelizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5555968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model = DressedQNN(qc_dev).to(device)\n",
      "    model = DDP(model)\n",
      "    torch.cuda.set_device(dp_info[\"local_rank\"])\n",
      "    model.cuda(dp_info[\"local_rank\"])\n"
     ]
    }
   ],
   "source": [
    "!sed -n 105,108p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b087bd",
   "metadata": {},
   "source": [
    "The above are the programming change you need to make to use data parallelism. In QML, we often want to save models, save results and print training progress. If each GPU executes the saving and printing command, the log would be flooded with the repeated information, and the model and results would be overwriting each other. To avoid this, we only save and print from the GPU that has `rank` 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4ccdfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    if dp_info[\"rank\"]==0:    \n",
      "        print('elapsed time: ', elapsed)\n",
      "        torch.save(model.state_dict(), f\"{output_dir}/test_local.pt\")\n",
      "        save_job_result({\"last loss\": loss_before})\n"
     ]
    }
   ],
   "source": [
    "!sed -n 134,137p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cf814",
   "metadata": {},
   "source": [
    "## Training with multiple GPUs in single instance\n",
    "With the modified algorithm, we can now submit our job with data parallelism. The SageMaker distributed data parallel library only supports three instance types listed below. Be sure to choose the instance type from the list and configure it through the `InstanceConfig` argument in Jobs. \n",
    "- ml.p3.16xlarge\n",
    "- ml.p3dn.24xlarge\n",
    "- ml.p4d.24xlarge\n",
    "\n",
    "For the SageMaker distributed data parallel library to know that data parallelism is enabled, we provide two additional hyperparameters for the library. `\"sagemaker_distributed_dataparallel_enabled\"` is set to `\"true\"`, and `\"sagemaker_instance_type\"` needs to be the instance type we are using. Keep in mind that data parallelism only works correctly when you modify your algorithm script according to the [previous section](#modify). If the data parallelism option is enabled in the hyperparameters without a correctly modified algorithm script, the job may throw errors, or each GPU may repeat the same workload without data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbb5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "instance_config = InstanceConfig(instanceType='ml.p3.16xlarge')\n",
    "\n",
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"32\", \n",
    "                 \"batch_size\": \"32\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.01\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                 \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
    "                 \"sagemaker_instance_type\": instance_config.instanceType,\n",
    "                }\n",
    "\n",
    "input_file_path = \"data/sonar.all-data\"\n",
    "\n",
    "image_uri=\"537332306153.dkr.ecr.us-west-2.amazonaws.com/lightning-bjob:v9\" # Remove this line after public container builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd73a4d",
   "metadata": {},
   "source": [
    "With the instance type and data parallelism configured, we can now submit our job. There are 8 GPUs in a `ml.p3.16xlarge` instance. When the instance spins up, the workload is distributed across the 8 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a121d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.gpu\",\n",
    "    source_module=\"source_script\",\n",
    "#     entry_point=\"source_script.train_dp_test\",\n",
    "    entry_point=\"source_script.train_dp\",\n",
    "    job_name=\"qml-dp1x-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    instance_config=instance_config,\n",
    "    image_uri=image_uri,\n",
    "    wait_until_complete=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should take about 6 minutes\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804b70c",
   "metadata": {},
   "source": [
    "## Training with multiple GPUs across multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e4e97",
   "metadata": {},
   "source": [
    "We are not limited to parallelizing the workload inside a single instance. We can perform distributed training by parallelizing our workload across multiple instances. With the algorithm script modified and the data parallelism enabled, we can perform distributed data parallelism by setting instance count larger than 1. To configure instance count, we use the `instanceCount` argument in `InstanceConfig`. The SageMaker distributed library we included in our algorithm script coordinates the multiple instances and conducts the distributed training for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ee06893",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_config = InstanceConfig(instanceType='ml.p3.16xlarge', instanceCount=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1163c",
   "metadata": {},
   "source": [
    "Be mindful that, when using multiple instances, each instance incurs charge based on how long you use it. In distributed data parallelism, when you set `instanceCount=2`, two instances are allocated to run your job. SageMaker distributed library managed the instances that they start and end at the same time. If your workload takes 200 seconds, you will be billed for 200 seconds for each instance used, which adds to 400 seconds in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"32\", \n",
    "                 \"batch_size\": \"32\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.01\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                 \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
    "                 \"sagemaker_instance_type\": instance_config.instanceType,\n",
    "                }\n",
    "\n",
    "input_file_path = \"data/sonar.all-data\"\n",
    "\n",
    "image_uri=\"537332306153.dkr.ecr.us-west-2.amazonaws.com/lightning-bjob:v9\" # Remove this line after public container builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7ef1a",
   "metadata": {},
   "source": [
    "Now we can submit our job with distributed data parallelism!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98994e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.gpu\",\n",
    "    source_module=\"source_script\",\n",
    "    entry_point=\"source_script.train_dp\",\n",
    "    job_name=\"qml-dp2x-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    instance_config=instance_config,\n",
    "    image_uri=image_uri,\n",
    "    wait_until_complete=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should take about 6 minutes\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d989c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we show you how to use SageMaker distributed library to parallelize quantum machine learning workloads. To learn more about distributed training, you can read the [Amazon Braket documentation](https://docs.aws.amazon.com/braket/latest/developerguide/braket-jobs.html) and [Amazon SageMaker Distributed Training Libraries](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html?tag=local002-20)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
