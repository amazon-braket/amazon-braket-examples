{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6938b0c",
   "metadata": {},
   "source": [
    "# How to data parallel\n",
    "- intro to qml\n",
    "- training with GPU\n",
    "- training with multi GPU\n",
    "- training with multi instances\n",
    "- input output\n",
    "- save and load model\n",
    "- eval accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee5c43",
   "metadata": {},
   "source": [
    "## Quantum machine learning\n",
    "Quantum machine learning (QML) is a special type of hybrid quantum-classical workload. Like classical machine learning (ML), there are usually a parameterized model, a dataset and a loss funcion. When training the model with the dataset, the parameters of the model are updated to minimize the loss funcion. In QML, the model contains one or many quantum circuits. The model may or may not also inlcude classical neural nets. A loss functions is usually defined for a single data point. Say a dataset $D$ has $N_D$ data points, $d_1$, $d_1$, ..., $d_{N_D}$. The losses associated with the data points are $L(d_1)$, $L(d_1)$, ..., $L(d_{N_D})$, where $L$ is the loss function. Without invoking any advanced feature, the algorithm script would compute these losses in serial, and then average them to be the total loss for gradient computations. This procedure is time consuming, especially when there are hundreds of data points. \n",
    "\n",
    "## Data parallelism\n",
    "The loss from one data point is independent to the other data points. The order of the loss evaluations therefore do not need to follow a specific order. They can even be evaluated <i>in parallel</i>! Losses and gradients of variational parameters associated with different data points can be evaluated on different GPU cores at the same time. This is known data parallelism. In this notebook, we will learn to use [SageMaker's distributed data parallel library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html) to accerlate the training of your quantum model. We go through examples to show you how to parallelize trainings across multiple GPUs in an instance, and even multiple GPUs over multiple instances! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabe652",
   "metadata": {},
   "source": [
    "## Binary Classification of Sonar dataset\n",
    "Let's use a binary classification of the [Sonar dataset](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29) as the QML example. The Sonar dataset contains 208 data points each with 60 features that are collected from sonar signals bouncing off materials. Each data points is either labeled as \"M\" for mines or \"R\" for rocks. Our QML model consists of an input layer, a quantum circuit and an output layer. The input and output layer are classical dense layer. The dimension of classical input layer is $60\\times N$, where $N$ is the number of qubit in the quantum circuit. The result of the input layer is encoded into the quantum circuit using [angle embedding](https://pennylane.readthedocs.io/en/stable/code/api/pennylane.AngleEmbedding.html). After the angle embedding, the quantum circuit has the same structure as figure 4 of [this paper](https://arxiv.org/abs/1804.00633). It is a generic circuit ansatz that has two parametrized [strongly entangle layers](https://pennylane.readthedocs.io/en/user-docs-refactor/code/pennylane.templates.layers.StronglyEntanglingLayer.html) and a single parametrized rotation gate at the first qubit. The measurement is only performed at the first qubit. Using the concept of classical ML, the dimension of the quantum circuit layer is $N\\times1$. The classical output layer has dimension $1\\times1$ which takes the measurment of the quantum circuit and outputs a real number. The loss function is the [margin loss function](https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html) between the model output and the label. See [model_def.py](source_script/model_def.py) and [quantum_circuit.py](source_script/quantum_circuit.py) for more detail about the model and the quantum circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd091b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"data/sonar.all-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b8dcf",
   "metadata": {},
   "source": [
    "## Training with single GPU core\n",
    "\n",
    "Let's start by running the job with `lightning.gpu` simulator on a single GPU core in a `ml.p3.2xlarge` instance which has one Nvidia V100 GPU core. The algorithm script to train our quanutm model is [train_single.py](source_script/train_single.py). In the algorithm script, we use Pennylane with PyTorch as our framework, which are both included in Braket's pre-configured PyTorch container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56669ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "instance_config = InstanceConfig(instanceType='ml.p3.2xlarge')\n",
    "\n",
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"16\", \n",
    "                 \"batch_size\": \"16\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.01\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                }\n",
    "\n",
    "input_file_path = \"data/sonar.all-data\"\n",
    "\n",
    "image_uri=\"537332306153.dkr.ecr.us-west-2.amazonaws.com/lightning-bjob:v9\" # Remove this line after public container builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4ecd5",
   "metadata": {},
   "source": [
    "We submit our job after setting up instance configuration, hyperparameters and the job container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330df7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.gpu\",\n",
    "    source_module=\"source_script\",\n",
    "    entry_point=\"source_script.train_single\",\n",
    "    job_name=\"dp-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    instance_config=instance_config,\n",
    "    image_uri=image_uri,\n",
    "    wait_until_complete=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e770028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last loss': 0.09291739761829376}\n"
     ]
    }
   ],
   "source": [
    "# This cell should take about 6 minutes\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819f81e",
   "metadata": {},
   "source": [
    "## Modify your algorithm script for data parallelism <a class=\"anchor\" id=\"modify\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf1fc8",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) and [TensorFlow](https://www.tensorflow.org/guide/distributed_training) have built-in features for data parallelism. With SageMaker's distributed data parallel library, Braket Jobs makes it easier for you to leverage data parallelism to accelerate your training. In this notebook we use apply data parallelism with PyTorch. To use data parallelism, you need to slightly modified your algorithm script. As an example, we modify the algorithm script [train_single.py](source_script/train_single.py) to [train_dp.py](source_script/train_dp.py). Let's go through the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6f33a",
   "metadata": {},
   "source": [
    "First, we import the `smdistributed` package which does most of the heavylifting for distributing your workloads accross multiple GPU and/or multiple instances. This package is pre-configured in the Braket PyTorch and TensorFlow containers. The `DDP` function from `smdistributed` converts the quantum model into a data parallelizable model. The `dist` module  tell our algorithm script the total number of instances for the training (`world_size`), and the `rank` and `local_rank` of a GPU core. `rank` is the absolute index of a GPU across all instances, while `local_rank` is the index of a GPU within an instance. For example, if there are four instances each with eight GPUs allocated for the trainin, the `rank` ranges from 0 to 31 and the `local_rank` ranges from 0 to 7.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f89264fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import smdistributed.dataparallel.torch.distributed as dist\n",
      "from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
      "    dp_info = {\n",
      "        \"world_size\": dist.get_world_size(),\n",
      "        \"rank\": dist.get_rank(),\n",
      "        \"local_rank\": dist.get_local_rank(),\n",
      "    }\n",
      "    batch_size //= dp_info[\"world_size\"] // 8\n",
      "    batch_size = max(batch_size, 1)\n",
      "    print(\"dp_info: \", dp_info)\n"
     ]
    }
   ],
   "source": [
    "!sed -n 23,24p source_script/train_dp.py\n",
    "!sed -n 65,72p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac7871",
   "metadata": {},
   "source": [
    "Next, we define a `DistributedSampler` according to the `world_size` and `rank`, and pass it into the data loader. This sampler avoids GPUs accessing the same slice of a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e849a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
      "        train_dataset, \n",
      "        num_replicas=dp_info[\"world_size\"], \n",
      "        rank=dp_info[\"rank\"]\n",
      "    )\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        train_dataset,\n",
      "        batch_size=batch_size,\n",
      "        shuffle=False,\n",
      "        num_workers=0,\n",
      "        pin_memory=True,\n",
      "        sampler=train_sampler,        \n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "!sed -n 81,93p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cb015",
   "metadata": {},
   "source": [
    "Next, we use the `DDP` fuction to make our quantum model parallelizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49b75555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model = DressedQNN(qc_dev).to(device)\n",
      "    model = DDP(model)\n",
      "    torch.cuda.set_device(dp_info[\"local_rank\"])\n",
      "    model.cuda(dp_info[\"local_rank\"])\n"
     ]
    }
   ],
   "source": [
    "!sed -n 105,108p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c6116",
   "metadata": {},
   "source": [
    "The above are the programming change you need to make to use data parallelism. In QML, it is very often that we want to save models, save results and print training progress. If each GPU executes the saving and printint command, the log would be flooded with the repeated information, and the model and results would be overwrting wach other. To avoid this, we only save and print from the GPU that has `rank` 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f36c61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    if dp_info[\"rank\"]==0:    \n",
      "        print('elapsed time: ', elapsed)\n",
      "        torch.save(model.state_dict(), f\"{output_dir}/test_local.pt\")\n",
      "        save_job_result({\"last loss\": loss_before})\n"
     ]
    }
   ],
   "source": [
    "!sed -n 134,137p source_script/train_dp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee45e92",
   "metadata": {},
   "source": [
    "## Training with multiple GPU cores in single instance\n",
    "With the modified algorithm, we can now submit our job with data parallelism. The SageMaker distributed data parallel library only supports three instance types listed below. Be sure to choose the instance type from the list and configure it through the `InstanceConfig` argument in Jobs. \n",
    "- ml.p3.16xlarge\n",
    "- ml.p3dn.24xlarge\n",
    "- ml.p4d.24xlarge\n",
    "\n",
    "For the SageMaker distributed data parallel library to know that data parallelism is enabled, we provide two additional hyperparameters for the library. `\"sagemaker_distributed_dataparallel_enabled\"` is set to `\"true\"`, and `\"sagemaker_instance_type\"` needs to be the instance type we are using. Keep in mind that data parallelism only works correctly when you modify your algorithm script according to the [previous section](#modify). If the data parallelism option is enabled in the hyperparameters without a correctly modified algorithm script, the job may throws errors, or each GPU may repeat the same workload without data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c5fc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "instance_config = InstanceConfig(instanceType='ml.p3.16xlarge')\n",
    "\n",
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"32\", \n",
    "                 \"batch_size\": \"32\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.01\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                 \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
    "                 \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
    "                }\n",
    "\n",
    "input_file_path = \"data/sonar.all-data\"\n",
    "\n",
    "image_uri=\"537332306153.dkr.ecr.us-west-2.amazonaws.com/lightning-bjob:v9\" # Remove this line after public container builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfab8d9",
   "metadata": {},
   "source": [
    "When the instance and data parallelism configured, we can now submit our job. There are 8 GPUs in a `ml.p3.16xlarge` instance. When the instance spins up, the workload is distributed across the 8 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646ae70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.gpu\",\n",
    "    source_module=\"source_script\",\n",
    "#     entry_point=\"source_script.train_dp_test\",\n",
    "    entry_point=\"source_script.train_dp\",\n",
    "    job_name=\"dp-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    instance_config=instance_config,\n",
    "    image_uri=image_uri,\n",
    "    wait_until_complete=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should take about 6 minutes\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9968d3",
   "metadata": {},
   "source": [
    "## Training with multiple GPU cores across multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96764e2f",
   "metadata": {},
   "source": [
    "We are not limited to parallelize the workload inisde a single instances. We can perform distributed training by parallizing our workload across multiple instances. With the algorithm script modified and the data parallism enabled, we can perform distributed data parallelism by setting instance count larger than 1. To configure instance count, we use the `instanceCount` argument in `InstanceConfig`. The SageMaker distributed library we included in our algorithm script coordinates the multiple instances and conducts the distributed training for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1b9dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_config = InstanceConfig(instanceType='ml.p3.16xlarge', instanceCount=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fb613",
   "metadata": {},
   "source": [
    "Be mindful that, when using multiple instances, each instance incurs charge based on how much time you use it. In distributed data parallelism, when you use set `instanceCount=2`, two instances are allocated to run your job. SageMaker distributed library managed the instances that they start and end at the same time. If your workload takes 200 seconds, you will be billed for 200 seconds for each instance used which adds to 400 seconds in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b932322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.config import InstanceConfig\n",
    "\n",
    "instance_config = InstanceConfig(instanceType='ml.p3.16xlarge')\n",
    "\n",
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"32\", \n",
    "                 \"batch_size\": \"32\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.01\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                 \"sagemaker_distributed_dataparallel_enabled\": \"true\",\n",
    "                 \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
    "                }\n",
    "\n",
    "input_file_path = \"data/sonar.all-data\"\n",
    "\n",
    "image_uri=\"537332306153.dkr.ecr.us-west-2.amazonaws.com/lightning-bjob:v9\" # Remove this line after public container builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a76a08",
   "metadata": {},
   "source": [
    "Now we can submit our job for distributed data parallelism!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bec3a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.gpu\",\n",
    "    source_module=\"source_script\",\n",
    "    entry_point=\"source_script.train_dp\",\n",
    "    job_name=\"dp-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    instance_config=instance_config,\n",
    "    image_uri=image_uri,\n",
    "    wait_until_complete=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should take about 6 minutes\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d4955",
   "metadata": {},
   "source": [
    "## Debug with local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa23250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\"nwires\": \"10\", \n",
    "                 \"ndata\": \"16\", \n",
    "                 \"batch_size\": \"16\", \n",
    "                 \"epochs\": \"5\", \n",
    "                 \"gamma\": \"0.99\", \n",
    "                 \"lr\": \"0.03\",\n",
    "                 \"seed\": \"164\",\n",
    "                 \"to_eval\": \"0\",\n",
    "                }\n",
    "input_file_path = \"data/sonar.all-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84dad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the short-lived AWS credentials found in session. They might expire while running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boto3 Version:  1.20.10\n",
      "Beginning Setup\n",
      "Running Code As Subprocess\n",
      "hyperparams:  {'nwires': '10', 'ndata': '16', 'batch_size': '16', 'epochs': '5', 'gamma': '0.99', 'lr': '0.03', 'seed': '164', 'to_eval': '0'}\n",
      "Using local simulator:  Lightning Qubit PennyLane plugin\n",
      "[2022-04-15 20:33:24.097 bf620bd46683:51 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-04-15 20:33:24.132 bf620bd46683:51 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Train Epoch: 1 [0/16 (0%)]\tLoss: 0.191733\n",
      "Metrics - timestamp=1650054804.4977372; Loss=0.19173291325569153; iteration_number=1;\n",
      "Train Epoch: 2 [0/16 (0%)]\tLoss: 0.200744\n",
      "Metrics - timestamp=1650054804.7766159; Loss=0.20074376463890076; iteration_number=2;\n",
      "Train Epoch: 3 [0/16 (0%)]\tLoss: 0.120848\n",
      "Metrics - timestamp=1650054805.1639974; Loss=0.12084810435771942; iteration_number=3;\n",
      "Train Epoch: 4 [0/16 (0%)]\tLoss: 0.102050\n",
      "Metrics - timestamp=1650054805.4387467; Loss=0.1020495668053627; iteration_number=4;\n",
      "Train Epoch: 5 [0/16 (0%)]\tLoss: 0.107891\n",
      "Metrics - timestamp=1650054805.7171986; Loss=0.10789139568805695; iteration_number=5;\n",
      "elapsed time:  1.9083030223846436\n",
      "Training Successful!!\n",
      "Code Run Finished\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from braket.jobs.local.local_job import LocalQuantumJob\n",
    "\n",
    "# This cell should take about 2 min for the fisrt time, and about 30 seconds afterward.\n",
    "job = LocalQuantumJob.create(\n",
    "    device=\"local:pennylane/lightning.qubit\",\n",
    "    source_module=\"source_script\",\n",
    "    entry_point=\"source_script.train_single\",\n",
    "    job_name=\"dp-\" + str(int(time.time())),\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data={\"input-data\": input_file_path},\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe88e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last loss': 0.10789139568805695}\n"
     ]
    }
   ],
   "source": [
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8606713",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook we showed you how to get started with running simulators embedded in Hybrid Jobs. To learn more, you can read the [documentation](https://docs.aws.amazon.com/braket/latest/developerguide/braket-jobs.html) or follow the other example notebooks in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dced2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall --yes amazon-braket-sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/aws/amazon-braket-sdk-python.git@local-sim-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d3ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws configure add-model --service-model \"file://braket-2019-09-01.normal.json\" --service-name braket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb1c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
