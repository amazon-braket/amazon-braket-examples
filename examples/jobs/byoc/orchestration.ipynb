{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38c7651",
   "metadata": {},
   "source": [
    "# Quantum Classifier with Custom Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c452c6c",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to use Amazon Braket Hybrid Job to train a quantum machine learning model. To run a hybird job, we need to prepare an algorithm script and an orchestration script. The algorithm script defines problem, machine learning model and metrics. The orchestration script defines the environment and the backend for the algorithm script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea57ea",
   "metadata": {},
   "source": [
    "## 1 Prepare Algorithm Script\n",
    "The purpose of this section is to show how to prepare the algorithm script for a text sentiment classifier. The complete script for this notebook is [here](algorithm_script.py). We will not go through the script line by line. Instead, we highlight the part that is important for understanding the hybrid job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a381592",
   "metadata": {},
   "source": [
    "### Problem Setup\n",
    "The quantum machine learning problem we are targeting is to classify the postive and negative sentiment of a sentence. We use four sentences in this example:\n",
    "\n",
    "\"I eat a banana everyday.\" <br>\n",
    "\"Banana is not for her.\" <br>\n",
    "\"Banana shake is delicious.\" <br>\n",
    "\"How can you like to eat bananas?\" <br>\n",
    "\n",
    "The first and the third sentences have positive sentiment on bananas, which are labeled as +1. The second and the fourth one have negative sentiment, which are labeled as -1. To input the sentence to a quantum machine learning model, we use spaCy package to embed the sentences into 1D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da91e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_sentence_bert\n",
    "\n",
    "nlp = spacy_sentence_bert.load_model(\"xx_distiluse_base_multilingual_cased_v2\")\n",
    "banana_string = [\"I eat a banana everyday.\",\n",
    "                \"Banana is not for her.\",\n",
    "                \"Banana shake is delicious.\",\n",
    "                \"How can you like to eat bananas?\"\n",
    "]\n",
    "banana_embeding = [nlp(d) for d in banana_string]\n",
    "data = [d.vector for d in banana_embeding]\n",
    "label = [1, -1, 1, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f8014",
   "metadata": {},
   "source": [
    "With \"xx_distiluse_base_multilingual_cased_v2\" language model, each data point is now a vector with length 512. See the [spaCy page](https://spacy.io/universe/project/spacy-sentence-bert) for details. Note that the size of the embeding vectors depends on the language model. When choosing a different model, we would expect a different shape of embeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9f6be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (512,)\n",
      "data size: (512,)\n",
      "data size: (512,)\n",
      "data size: (512,)\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    print(\"data size: {}\".format(d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36dca6",
   "metadata": {},
   "source": [
    "### Quantum machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0d095",
   "metadata": {},
   "source": [
    "We choose [Circuit-centric quantum classifiers (CCQC)](https://arxiv.org/abs/1804.00633) as our quantum model. The figure below shows an example of CCQC circuit with 7 qubit. The data (classical embeding from the language model) is input to the circuit as the initial state via [amplitude encoding](https://arxiv.org/abs/1803.07128). Followed the initail states are two entanglement layers. The first one entangles each qubit with its nearest neighbor, while the second one with its thrid nearest neighbor. A rotation gate is then put at the first qubit. Finally, the measurment is only done at the first qubit. The classification criterion is only based on this measurement. If the measurement is >0, it predicts a postive sentiment (+1), otherwise it predicts a negative one (-1).\n",
    "\n",
    "\n",
    "<img src=\"ccqc_circuit.png\" alt=\"Image of quantum circuit\" width=\"600\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc98324",
   "metadata": {},
   "source": [
    "We use Pennylane as the machine learning framework. To use AWS QPU, we set the device name to be \"braket.aws.qubit\". As the algorithm script, we do not have to specify the device arn explcitly. We can assign it as <code>os.environ[\"AMZN_BRAKET_DEVICE_ARN\"]</code>, and it will be fetched from the orchestration script through environment variables. For details about options of Braket devices in <code>qml.device</code>, see [Amazon Braket-Pennylane Plugin](https://github.com/aws/amazon-braket-pennylane-plugin-python).\n",
    "<pre><code>\n",
    "dev = qml.device(\"braket.aws.qubit\",\n",
    "         device_arn = os.environ[\"AMZN_BRAKET_DEVICE_ARN\"],\n",
    "         wires=self.nwires,\n",
    "         s3_destination_folder = None,\n",
    "        )\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(*weights, features=np.zeros(2**nwires)):\n",
    "    # components of the CCQC circuit\n",
    "</pre></code>\n",
    "The quantum model is packaged in a CCQC class in the [algorithm script](algorithm_script.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792950f",
   "metadata": {},
   "source": [
    "### Monitor metrics and Record Results\n",
    "We can monitor the progress of the hybrid job in Amazon Braket console. <code>log_metric</code> records the metrics so that we can view the training progress in the \"Monitor\" console tab.  <code>save_job_result</code> allows us to view the result in the console and in the orchestration script. \n",
    "<pre><code>\n",
    "from braket.jobs import save_job_result\n",
    "from braket.jobs.metrics import log_metric\n",
    "\n",
    "log_metric(\n",
    "    metric_name=\"Cost\",\n",
    "    value=cost,\n",
    "    iteration_number=i,\n",
    ")\n",
    "\n",
    "weights = [weights.to;list()]\n",
    "save_job_result({\"weights\": weights})\n",
    "</pre></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e102c7",
   "metadata": {},
   "source": [
    "## 2 Prepare Custom Container\n",
    "Amazon Braket Jobs provides three pre-built containers for different use cases. If your applications fall outside of the support of these three pre-built containers, you have the option to build your own container. In this notebook, the spaCy package is not support either of the three containers, so we need to create our own!\n",
    "\n",
    "### Preparation 1: Docker \n",
    "To build and upload your custom container, you must have Docker installed. <br>\n",
    "\n",
    "### Preparation 2: Dockerfile\n",
    "This defines the environment and the software in the containers. We can start with the base dockerfile of Amazon Braket Hybrid Job as an example and add packages for our needs. For our quantum text classifier, we only need to add the following lines to the example dockerfile. The completed dockerfile for this excercise is [here](dockerfile).\n",
    "\n",
    "<pre><code>\n",
    "RUN ${PIP} install PennyLane==0.16.0 \\\n",
    "                   spacy==3.1.3 \\\n",
    "                   spacy_sentence_bert==0.1.2\n",
    "</pre></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903eab3d",
   "metadata": {},
   "source": [
    "### Preparation 3: Initial Script\n",
    "This contains the initial codes to run when your container starts. For this exercise, we can directly use braket_container.py from the example code without modification. The initial script for this excercise is [here](qml_source/braket_container.py).\n",
    "\n",
    "### Preparation 4: Create ECR\n",
    "Follow the [instruction](https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-create.html) to create a \"private\" container repository. For this excerise, we name our repository \"my-qtc\" (my quantum text classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50618cdc",
   "metadata": {},
   "source": [
    "### Action 1: Login to AWS CLI and Docker\n",
    "If you haven't already, follow the [instruction](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html) to configure your credential in AWS CLI. Then, run the following code to log in to Docker. Replace all [xxx] below with your own credentials. You will see \"Login Succeeded\" when it's done.\n",
    "<pre><code>\n",
    "aws ecr get-login-password --region [your_region] | docker login --username AWS --password-stdin [aws_account_id].dkr.ecr.[your_region].amazonaws.com\n",
    "</pre></code>\n",
    "If your terminal does not support interactive login, you can also run the following line to log in.\n",
    "<pre><code>\n",
    "docker login -u AWS -p $(aws ecr get-login-password --region [your_region]) [aws_account_id].dkr.ecr.[your_region].amazonaws.com\n",
    "</pre></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc10122",
   "metadata": {},
   "source": [
    "### Action 2: Build and push the image\n",
    "Go to the folder where you have docker and your initial script (braket_container.py in this case). Run the lines below to build and push the image to your ECR. Remember to replace all [xxx] in the codes with your own credentials. When it completes, you will see all layers are pushed in your terminal, and a new image will appear in your ECR console under \"my-qtc\" repository. If you run into memory error when building an image due to the size of the language model, increase the memory limit in Docker. \n",
    "<pre><code>\n",
    "docker build -t dockerfile .\n",
    "docker tag dockerfile:latest [aws_account_id].dkr.ecr.[your_region].amazonaws.com/my-qtc:latest\n",
    "docker push [aws_account_id].dkr.ecr.[your_region].amazonaws.com/my-qtc:latest\n",
    "</pre></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8152f8",
   "metadata": {},
   "source": [
    "## 3 Orchestration Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79bf802",
   "metadata": {},
   "source": [
    "### Local Job\n",
    "Before submit job to aws, you can test the job locally by using <code>LocalQuantumJob</code>. This help debuging and configuring the environment. Remember to provide the container we just created via image_uri keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6732bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.jobs.local.local_job import LocalQuantumJob\n",
    "\n",
    "image_uri = \"[aws_account_id].dkr.ecr.[your_region].amazonaws.com/my-qtc:latest\"\n",
    "\n",
    "job = LocalQuantumJob.create(\n",
    "    \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\",\n",
    "    source_module=\"qml_source\",\n",
    "    entry_point=\"qml_source.algorithm_script:main\",\n",
    "    wait_until_complete=True,\n",
    "    image_uri = image_uri,\n",
    "    job_name = \"my-qtc-job\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933de1c4",
   "metadata": {},
   "source": [
    "### Runing Hybrid Job in AWS\n",
    "Now we have prepared an algorithm script and the container for the job, we can submit the hybrid job to AWS using <code>AwsQuantumJob</code>. Remember to provide the container we just created via <code>image_uri</code> keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38b86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "image_uri = \"[aws_account_id].dkr.ecr.[your_region].amazonaws.com/my-qtc:latest\"\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\",\n",
    "    source_module=\"qml_source\",\n",
    "    entry_point=\"qml_source.algorithm_script:main\",\n",
    "    wait_until_complete=True,\n",
    "    image_uri = image_uri,\n",
    "    job_name = \"my-qtc-job\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbf94a",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Now the training is completed, we can evaulate how our quantum model performs. First, we initialize the CCQC model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0a3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_source.algorithm_script import CCQC\n",
    "\n",
    "qml_model = CCQC(nwires = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa70c87",
   "metadata": {},
   "source": [
    "We then retrieved the trained weights from <code>job.result()</code>. If you lose the <code>job</code> variable, you can always retreive it by its arn which can be found in your console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9d87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.aws import AwsQuantumJob\n",
    "\n",
    "job_arn = \"your-job-arn\"\n",
    "job = AwsQuantumJob(job_arn)\n",
    "weights = job.result()['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a26bfc",
   "metadata": {},
   "source": [
    "Using the trained weight, we can make the prediction for each sentence with the <code>predict_fun</code> of our model. See the [algorithm script](algorithm_script.py) for definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6552295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat a banana everyday.\n",
      "label: 1  predict:1\n",
      "\n",
      "Banana is not for her.\n",
      "label: -1  predict:-1\n",
      "\n",
      "Banana shake is delicious.\n",
      "label: 1  predict:1\n",
      "\n",
      "How can you like to eat bananas?\n",
      "label: -1  predict:-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(banana_string[i])\n",
    "    pred = qml_model.predict_fun(*weights, data=data[i])\n",
    "    print(\"label: {}  predict:{}\".format(label[i], pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a123e4",
   "metadata": {},
   "source": [
    "We can also test our quantum model on a sentence it has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24cbf297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A banana a day keeps the doctor away.\n",
      "label: 1  predict:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_string = \"A banana a day keeps the doctor away.\"\n",
    "test_data = nlp(test_string).vector\n",
    "test_label = 1\n",
    "print(test_string)\n",
    "pred = qml_model.predict_fun(*weights, data=test_data)\n",
    "print(\"label: {}  predict:{}\".format(test_label, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
