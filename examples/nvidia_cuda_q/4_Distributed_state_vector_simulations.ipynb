{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021cfd04-df68-414d-a864-48f62fc8ddfb",
   "metadata": {},
   "source": [
    "# Distributed state vector simulations on multiple GPUs (advanced)\n",
    "\n",
    "In the notebook \"3_Multiple_GPU_simulations.ipynb\", you learned how to use CUDA-Q and Braket Hybrid Jobs to parallelize the simulation of a batch of observables and circuits over multiple GPUs, where each GPU simulates a single QPU. For workloads with larger qubit counts, however, it may be necessary to distribute a single state vector simulation across multiple GPUs so that multiple GPUs together simulate a single QPU.\n",
    "\n",
    "In this notebook, you will learn how to use CUDA-Q and Braket Hybrid Jobs to tackle this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b46659-6dcc-4900-a13a-e971f8bf0590",
   "metadata": {},
   "source": [
    "We start with necessary imports that are used in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8738f65f-969c-4b58-96f8-69bbc1bad5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braket.aws import AwsSession\n",
    "from braket.jobs import hybrid_job\n",
    "from braket.jobs.config import InstanceConfig\n",
    "from braket.jobs.image_uris import Framework, retrieve_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331136a-a369-4eef-8bfe-252c79103a3e",
   "metadata": {},
   "source": [
    "For this example, we need to use the PyTorch hybrid jobs container provided by Braket, which contains both CUDA-Q and the underlying CUDA support required for distributing our computation across multiple GPUs. Note: this container image is different from the one used in the previous notebooks illustrating more basic CUDA-Q scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f552c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = retrieve_image(Framework.CUDAQ, AwsSession().region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844344d-0978-4b11-8fe4-66b387e80c72",
   "metadata": {},
   "source": [
    "## Distributed state vector simulations\n",
    "Now that we have the container image URI, we are ready to run our workload. The `nvidia` target with the `mgpu` option supports distributing state vector simulations to multiple GPUs. This enables GPU simulations for circuits with higher qubit counts, up to 34 qubits. The example below shows how to submit a job with the `mgpu` option. Note, the `ml.g4dn.12xlarge` has 4 GPUs, the other supported `g4dn` instances only have a single GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38b73de-6a7e-45b7-b64b-afec60c0a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ARN:  arn:aws:braket:us-east-1:641737106670:job/6a72cfcf-8130-40f7-9db6-25da2ecc467e\n"
     ]
    }
   ],
   "source": [
    "@hybrid_job(\n",
    "    device=\"local:nvidia/nvidia-mgpu\",\n",
    "    instance_config=InstanceConfig(instanceType=\"ml.g4dn.12xlarge\", instanceCount=1),\n",
    "    image_uri=image_uri,\n",
    "    distribution = \"mpi\",\n",
    ")\n",
    "def distributed_gpu_job(\n",
    "    n_qubits,\n",
    "    n_shots,\n",
    "):\n",
    "    import os\n",
    "\n",
    "    import cudaq\n",
    "    \n",
    "    # Check environment variables\n",
    "    print(\"=== Environment Check ===\")\n",
    "    print(f\"OMPI_COMM_WORLD_SIZE: {os.getenv('OMPI_COMM_WORLD_SIZE', 'Not set')}\")\n",
    "    print(f\"OMPI_COMM_WORLD_RANK: {os.getenv('OMPI_COMM_WORLD_RANK', 'Not set')}\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.getenv('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "    \n",
    "    cudaq.set_target(\"nvidia\", option=\"mgpu\")\n",
    "    print(\"CUDA-Q backend:\", cudaq.get_target())\n",
    "    print(\"num_available_gpus:\", cudaq.num_available_gpus())\n",
    "    \n",
    "    # Check MPI status\n",
    "    print(\"=== MPI Status ===\")\n",
    "    try:\n",
    "        print(f\"MPI initialized: {cudaq.mpi.is_initialized()}\")\n",
    "        if cudaq.mpi.is_initialized():\n",
    "            print(f\"MPI rank: {cudaq.mpi.rank()}\")\n",
    "            print(f\"MPI num_ranks: {cudaq.mpi.num_ranks()}\")\n",
    "        else:\n",
    "            print(\"MPI not initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"MPI check failed: {e}\")\n",
    "\n",
    "    # Define target\n",
    "    cudaq.set_target(\"nvidia\", option=\"mgpu\")\n",
    "    print(\"CUDA-Q backend: \", cudaq.get_target())\n",
    "    print(\"num_available_gpus: \", cudaq.num_available_gpus())\n",
    "\n",
    "    # Initialize MPI and view the MPI properties\n",
    "    # cudaq.mpi.initialize()\n",
    "    # rank = cudaq.mpi.rank()\n",
    "\n",
    "    # Define circuit and observables\n",
    "    @cudaq.kernel\n",
    "    def ghz():\n",
    "        qubits = cudaq.qvector(n_qubits)\n",
    "        h(qubits[0])\n",
    "        for q in range(1, n_qubits):\n",
    "            cx(qubits[0], qubits[q])\n",
    "\n",
    "    hamiltonian = cudaq.SpinOperator.random(n_qubits, 1)\n",
    "\n",
    "    # Parallelize circuit simulation\n",
    "    result = cudaq.observe(ghz, hamiltonian, shots_count=n_shots)\n",
    "\n",
    "    # End the MPI interface\n",
    "    # cudaq.mpi.finalize()\n",
    "\n",
    "    # if rank == 0:\n",
    "    return {\"expectation\": result.expectation()}\n",
    "\n",
    "\n",
    "n_qubits = 25\n",
    "n_shots = 1000\n",
    "distributed_job = distributed_gpu_job(n_qubits, n_shots)\n",
    "print(\"Job ARN: \", distributed_job.arn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a054c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expectation': 0.054000000000000034}\n",
      "result: 0.054000000000000034\n"
     ]
    }
   ],
   "source": [
    "distributed_job_result = distributed_job.result()\n",
    "print(distributed_job_result)\n",
    "print(f\"result: {distributed_job_result['expectation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18004fe8-24a0-4316-ab0a-a4e0aac6ba1e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook shows you how to distribute a single state vector simulation across multiple GPUs so that multiple GPUs together simulate a single QPU. If you have workloads with a qubit count that is too large to simulate on a single GPU, you can use this technique to make these large workloads feasible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
